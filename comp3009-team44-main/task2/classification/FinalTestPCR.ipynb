{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCR Classification Test File\n",
    "\n",
    "This notebook will allow the assessor to test our classification model on unseen test data. Simply run all cells in order. You will have to change the path to your file here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"../testDataSetExample.xls\" # path to the unseen dataset\n",
    "TRAIN_PATH = \"../trainDataset.csv\" # path to the provided csv we uploaded to moodle\n",
    "RESULTS_PATH = \"../resultsPCR.csv\" # path to the location in which you wish to save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can handle xls imports only if the xlrd dependency is installed:\n",
    "\n",
    "`$conda install xlrd`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training data. This process is explained in more detail in other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "df = df[~(df == 999).any(axis=1)]\n",
    "df.drop('ID', axis=1, inplace=True)\n",
    "for i in range(12, len(df.columns)):\n",
    "    df.columns.values[i] = 'img_' + str(i)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "threshold = 4 # standard deviations from mean (99.7% of data)\n",
    "for i, col_name in enumerate(df.columns[13:]):\n",
    "    col = df[col_name]\n",
    "    mean = np.mean(col)\n",
    "    std = np.std(col)\n",
    "    for j, x in enumerate(col):\n",
    "        z = (x - mean) / std\n",
    "        if z > threshold:\n",
    "            df.iloc[j, i] = mean\n",
    "\n",
    "# Assign features to X\n",
    "X = df.drop('pCR (outcome)', axis=1).drop('RelapseFreeSurvival (outcome)', axis=1).drop('TrippleNegative', axis = 1)\n",
    "X.drop('index', axis=1, inplace=True)\n",
    "\n",
    "# Assign labels to y\n",
    "y = df['pCR (outcome)']\n",
    "X.head()\n",
    "\n",
    "# Scale test data to match training data\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA on image data, choosing a number of components such that we retain 90% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N components: 14\n"
     ]
    }
   ],
   "source": [
    "feature_names = list(X.columns) \n",
    "pca = PCA(n_components=0.90) # retain 90% of variance\n",
    "img_pca = pca.fit_transform(Xs[:,9:]) #this number is 9 because after removing index, TrippleNegtaive, pCR outcome, RFS, that is the index of img_12  \n",
    "print(\"N components:\",pca.n_components_)\n",
    "\n",
    "keep = 10\n",
    "img_pca = img_pca[:,:keep] # retain the first 10 components\n",
    "cols = ['pca_' + str(i+1) for i in range(keep)]\n",
    "df_img_pca = pd.DataFrame(img_pca, columns=cols)\n",
    "\n",
    "col_names = X.columns[:9]#this number is 9 because after removing index, TrippleNegtaive, pCR outcome, RFS, that is the index of img_12  \n",
    "Xs_pca = pd.concat([pd.DataFrame(Xs[:,:9], columns=col_names), df_img_pca], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SMOTE (minority class oversampling) to boost population of minority class for better testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_pca_resampled, y_resampled = SMOTE().fit_resample(Xs_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with optimal parameters found in testing (SVM: C=10, Gamma=0.1, Kernel=RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=50, gamma=0.1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(C=50, gamma=0.1, kernel=\"rbf\")\n",
    "model.fit(Xs_pca_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare testing data in the same manner as training data, not including SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel(TEST_PATH)\n",
    "\n",
    "test_df = test_df[~(test_df == 999).any(axis=1)]\n",
    "ids = test_df.iloc[:,0]\n",
    "test_df.drop('ID', axis=1, inplace=True)\n",
    "for i in range(12, len(test_df.columns)):\n",
    "    test_df.columns.values[i] = 'img_' + str(i)\n",
    "\n",
    "test_df = test_df.reset_index()\n",
    "\n",
    "threshold = 4 # standard deviations from mean (99.7% of data)\n",
    "for i, col_name in enumerate(test_df.columns[13:]):\n",
    "    col = test_df[col_name]\n",
    "    mean = np.mean(col)\n",
    "    std = np.std(col)\n",
    "    for j, x in enumerate(col):\n",
    "        z = (x - mean) / std\n",
    "        if z > threshold:\n",
    "            test_df.iloc[j, i] = mean\n",
    "\n",
    "# Assign features to X\n",
    "X = test_df.drop('TrippleNegative', axis = 1)\n",
    "X.drop('index', axis=1, inplace=True)\n",
    "\n",
    "# Scale test data to match training data\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N components: 14\n"
     ]
    }
   ],
   "source": [
    "# Use PCA for dimensionality reduction\n",
    "feature_names = list(X.columns) \n",
    "pca = PCA(n_components=14) # retain 90% of variance\n",
    "img_pca = pca.fit_transform(Xs[:,9:]) #this number is 9 because after removing index, TrippleNegtaive, pCR outcome, RFS, that is the index of img_12  \n",
    "print(\"N components:\",pca.n_components_)\n",
    "\n",
    "keep = 10\n",
    "img_pca = img_pca[:,:keep] # retain the first 5 components\n",
    "cols = ['pca_' + str(i+1) for i in range(keep)]\n",
    "df_img_pca = pd.DataFrame(img_pca, columns=cols)\n",
    "\n",
    "col_names = X.columns[:9]#this number is 9 because after removing index, TrippleNegtaive, pCR outcome, RFS, that is the index of img_12  \n",
    "Xs_pca = pd.concat([pd.DataFrame(Xs[:,:9], columns=col_names), df_img_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/09/n64g0frn7qzbttkts3f1pzth0000gn/T/ipykernel_81380/1270018350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PCR (Prediction)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SAVE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(Xs_pca)\n",
    "\n",
    "output = pd.DataFrame(columns=['ID', 'PCR (Prediction)'])\n",
    "output['ID'] = ids\n",
    "output['PCR (Prediction)'] = y_predict\n",
    "\n",
    "output.to_csv(RESULTS_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9d512b7675538f6760a984fe58da4b3bd8085ea6c5aba7a1079338112a56ef9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
